import streamlit as st
import pandas as pd
import json
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
from openai import OpenAI
import ast
import nltk

nltk.download('punkt')

# Aseg칰rate de configurar tus claves de API de OpenAI en `st.secrets`
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])


#def get_embedding(text, model="text-embedding-ada-002"):
#    text = text.replace("\n", " ")
#    response = client.embeddings.create(input=[text], model=model)

#    return response['data'][0]['embedding']
# Funci칩n para obtener embeddings de texto utilizando un modelo de OpenAI.
def get_embedding(text, model="text-embedding-ada-002"):
    text = text.replace("\n", " ")# Reemplaza saltos de l칤nea por espacios para evitar errores en el procesamiento.
    return client.embeddings.create(input=[text], model=model).data[0].embedding # Llama a la API de OpenAI para obtener el embedding del texto, retorna el embedding
# Funci칩n para calcular la distancia cosina entre dos vectores.
def cosine_distance(v1, v2):
    dot_product = sum(a*b for a, b in zip(v1, v2))
    magnitude_v1 = sum(a*a for a in v1) ** 0.5
    magnitude_v2 = sum(a*a for a in v2) ** 0.5
    if magnitude_v1 == 0 or magnitude_v2 == 0:
        return 1
    return 1 - dot_product / (magnitude_v1 * magnitude_v2)
# Funci칩n para crear un contexto basado en la pregunta del usuario, seleccionando tr치mites relevantes.
def create_context(question, df):# Obtiene el embedding de la pregunta.
    q_embedding = get_embedding(question)
    df['distance'] = df['embedding'].apply(lambda emb: cosine_distance(q_embedding, emb))
    sorted_df = df.sort_values('distance')# Ordena los tr치mites por su distancia (relevancia).
    relevant_tramites = sorted_df['Tr치mite'].unique()[:5]# Selecciona los 5 tr치mites m치s relevantes.
    return relevant_tramites




#def calculate_cosine_distance(emb1, emb2):
#    return spatial.distance.cosine(emb1, emb2)
def build_context_for_selected_tramite(df, tramite_elegido, max_len=3000):
    # Selecciona todas las entradas para el tr치mite elegido.
    df_filtrado = df[df['Tr치mite'] == tramite_elegido]
    context = ""
    total_len = 0
    for _, row in df_filtrado.iterrows():
        text_len = len(row['Combined'].split())
        if total_len + text_len > max_len:
            break
        context += row['Combined'] + "\n\n"
        total_len += text_len
    return context

#def create_context(question, df):
 #   q_embedding = get_embedding(question)
 #   df['distance'] = df['embedding'].apply(lambda emb: calculate_cosine_distance(q_embedding, emb))
 #   sorted_df = df.sort_values('distance')
 #   relevant_tramites = sorted_df['Tr치mite'].unique()[:3]
 #   return relevant_tramites
# Funci칩n para construir el contexto para un tr치mite seleccionado.
#def build_context_for_selected_tramite(df, tramite_elegido, max_len=3000, pregunta=None):
#    Filtra el DataFrame por tr치mite seleccionado, y opcionalmente por pregunta.
#    if pregunta:
#        df_filtrado = df[(df['Tr치mite'] == tramite_elegido) #& (df['Pregunta_Completa'] == pregunta)]
#    else:
#        df_filtrado = df[df['Tr치mite'] == tramite_elegido]
#    context = "" # Inicializa el contexto como un string vac칤o.
#    total_len = 0 # Inicializa el contador de longitud total del contexto.
#    for _, row in df_filtrado.iterrows():
#        text_len = len(row['Combined'].split()) # Calcula la longitud del texto actual.
#        if total_len + text_len > max_len: # Verifica si agregar este texto superar칤a el l칤mite de longitud.
#            break
#        context += row['Combined'] + "\n\n" # Agrega el texto al contexto.
#        total_len += text_len # Retorna el contexto construido.
#    return context


# Funci칩n para generar respuestas a preguntas utilizando el contexto y el modelo de OpenAI.

def answer_questions(questions, context="", model="gpt-4", max_tokens=500):
    responses = [] # Inicializa una lista para almacenar las respuestas.
    # Itera sobre cada pregunta para generar una respuesta.
    for question in questions:
        try:
            # Genera una respuesta utilizando el modelo de OpenAI y el contexto proporcionado.
            response = client.chat.completions.create( 
                model=model,
                messages=[
                    {"role": "system", "content": "Eres un especialista en temas fiscales en M칠xico. Vas a responder consultas sobre los tr치mites del anexo 1-A de la Resoluci칩n Miscel치nea Fiscal 2023 de M칠xico responde de forma amable, si la respuesta consiste en m칰ltiples pasos o documentos responde haciendo una lista de la informaci칩n. Si el usuario ingreso texto en  Ingresa tu pregunta o describe el tr치mite que te interesa, responde basado en todos los embeddings del tr치mite seleccionado"},
                    {"role": "user", "content": f"Context: {context}\n\n---\n\nQuestion: {question}\nAnswer:"}
                ],
                temperature=0.5,
                max_tokens=max_tokens,
                top_p=1.0,
                frequency_penalty=0.0,
                presence_penalty=0.0,
            )
            # Extrae la respuesta generada y la a침ade a la lista de respuestas.
            text_response = response.choices[0].message.content
            responses.append(text_response.strip())
        except Exception as e:
            print(e)
            responses.append("")
    return responses

def main():
    st.title("Asistente para Tr치mites Fiscales")

    df = pd.read_csv("./data/TODOStramitesembdf_modificadoVF.csv")

    # df2 = pd.read_csv("./data/tramitespreguntasEMBVFfundamentoVF.csv")
     #df['Pregunta_Completa'] = df2['Pregunta_Completa']
     #df['Fundamento jur칤dico'] = df2['Fundamento jur칤dico']

     #df['embedding'] = df['embedding'].astype(str).apply(ast.literal_eval)
    df['embedding'] = df['embedding'].astype(str).apply(eval)
    #df['embedding'] = df['embedding'].apply(lambda emb: json.loads(emb) if isinstance(emb, str) else emb)

    busqueda_opcion = st.radio("쮺칩mo deseas buscar?", ('Por Fundamento Jur칤dico', 'Describir Tr치mite o Pregunta'))

    if busqueda_opcion == 'Por Fundamento Jur칤dico':
        df_filterderogado = df[~df['Fundamento jur칤dico'].str.contains("Tr치mite derogado", na=False)]
        fundamentos_legales = df_filterderogado['Fundamento jur칤dico'].dropna().unique()
        fundamento_seleccionado = st.selectbox("Selecciona el fundamento legal de tu inter칠s:", [''] + list(fundamentos_legales))
        pass

        if fundamento_seleccionado:
            tramites_asociados = df_filterderogado[df_filterderogado['Fundamento jur칤dico'] == fundamento_seleccionado]['Tr치mite'].unique()
            tramite_seleccionado = st.selectbox("Selecciona el tr치mite asociado:", [''] + list(tramites_asociados))

    else:
        question = st.text_input("Ingresa tu pregunta o describe el tr치mite que te interesa:")
        if question:
            relevant_tramites = create_context(question, df)
            tramite_elegido = st.selectbox("Tr치mites sugeridos basados en tu descripci칩n:", relevant_tramites)

    if 'tramite_seleccionado' in locals() or 'tramite_elegido' in locals():
        selected_tramite = tramite_seleccionado if 'tramite_seleccionado' in locals() else tramite_elegido
        preguntas_df = df[df['Tr치mite'] == selected_tramite]
        preguntas = preguntas_df['Pregunta_Completa'].dropna().unique()
        
        pregunta_seleccionada = st.selectbox("Selecciona una pregunta de tu inter칠s:", [''] + list(preguntas), key='pregunta_seleccionada')        
        pregunta_seleccionada_index = -1 if pregunta_seleccionada == '' else preguntas.tolist().index(pregunta_seleccionada)
    #if 'tramite_seleccionado' in locals() or 'tramite_elegido' in locals():
    #    selected_tramite = tramite_seleccionado if 'tramite_seleccionado' in locals() else tramite_elegido
    #    preguntas_df = df[df['Tr치mite'] == selected_tramite]
    #    preguntas = preguntas_df['Pregunta_Completa'].dropna().unique()
    #    pregunta_seleccionada = st.selectbox("Selecciona una pregunta de tu inter칠s:", [''] + list(preguntas))
  
        nueva_pregunta = "" #st.text_input("Ingresa tu nueva pregunta aqu칤:")
        if pregunta_seleccionada_index == -1:
            nueva_pregunta = st.text_input("Ingresa tu nueva pregunta aqu칤:", key='nueva_pregunta')
            #nueva_pregunta = st.text_input("Ingresa tu nueva pregunta aqu칤:", key='nueva_pregunta')

        if st.button("Obtener Respuesta"):
            pregunta_final = nueva_pregunta if nueva_pregunta else pregunta_seleccionada
            if pregunta_final:
                context = build_context_for_selected_tramite(df, selected_tramite)
                response = answer_questions([pregunta_final], context=context)[0]
                st.text_area("Respuesta:", value=response, height=150)
        #if st.button("Obtener Respuesta"):
        #    pregunta_final = nueva_pregunta if nueva_pregunta else pregunta_seleccionada
        #    if pregunta_final:
        #        # Ahora se construye el contexto solo con el tr치mite seleccionado, sin considerar directamente la pregunta.
        #        context = build_context_for_selected_tramite(df, selected_tramite)
        #        response = answer_questions([pregunta_final], context=context)[0]
        #        st.text_area("Respuesta:", value=response, height=150)

    #if 'tramite_seleccionado' in locals() or 'tramite_elegido' in locals():
    #    selected_tramite = tramite_seleccionado if 'tramite_seleccionado' in locals() else tramite_elegido
    #    preguntas_df = df[df['Tr치mite'] == selected_tramite]
    #    preguntas = preguntas_df['Pregunta_Completa'].dropna().unique()
    #    pregunta_seleccionada = st.selectbox("Selecciona una pregunta de tu inter칠s o escribe una nueva abajo:", [''] + list(preguntas))

    #    if pregunta_seleccionada == '':
    #        nueva_pregunta = st.text_input("Ingresa tu nueva pregunta aqu칤:")
    #    else:
     #       nueva_pregunta = ''
        
        #nueva_pregunta = st.text_input("O ingresa tu nueva pregunta aqu칤:")
        
        #if st.button("Obtener Respuesta"):
        #    pregunta_final = nueva_pregunta if nueva_pregunta else pregunta_seleccionada
        #    if pregunta_final:
        #        context = build_context_for_selected_tramite(df, selected_tramite, pregunta=pregunta_final)
        #        response = answer_questions([pregunta_final], context=context)[0]
        #        st.text_area("Respuesta:", value=response, height=150)

if __name__ == "__main__":
    main()



#App
#def main():
#    st.set_page_config(page_title="Chat de Tr치mites Fiscales", page_icon="游늹")
#    st.sidebar.header("Asistente de  IA para Tr치mites Fiscales")
#    st.title("IA Tr치mites Fiscales")
#    st.header(':blue[Asistente de  IA para Tr치mites Fiscales]', divider='blue')
#   st.subheader(':grey[Nuestro asistente automatizado responde tus preguntas sobre tr치mites fiscales]', divider='blue')

#if "openai_model" not in st.session_state:
#    st.session_state["openai_model"] = "gpt-3.5-turbo"

#if "messages" not in st.session_state:
#    st.session_state.messages = []
#st.write(df)

#for message in st.session_state.messages:
#    with st.chat_message(message["role"]):
#        st.markdown(message["content"])

#if prompt := st.chat_input("What is up?"):
#    st.session_state.messages.append({"role": "user", "content": prompt})
#    with st.chat_message("user"):
#        st.markdown(prompt)

#    with st.chat_message("assistant"):
#        stream = client.chat.completions.create(
#            model=st.session_state["openai_model"],
 #           messages=[
 #               {"role": m["role"], "content": m["content"]}
 #               for m in st.session_state.messages
#            ],
 #           stream=True,
 #       )
#        response = st.write_stream(stream)
 #   st.session_state.messages.append({"role": "assistant", "content": response})
